"""
Complete TRM test with final optimized configuration.
Shows the full power of progressive refinement.
"""
import torch
from nanochat.gpt import GPT, GPTConfig

def main():
    print("="*80)
    print("TRM PROGRESSIVE REFINEMENT - FINAL TEST")
    print("="*80)
    print()

    torch.manual_seed(42)

    config = GPTConfig(
        sequence_len=64,
        vocab_size=100,
        n_layer=6,  # Deeper for more refinement capacity
        n_head=4,
        n_kv_head=4,
        n_embd=128,
        n_loops=2,
        n_sup_train=4,
        activation_fn='relu_squared'
    )

    print(f"Configuration:")
    print(f"  Layers: {config.n_layer}")
    print(f"  Loops per layer: {config.n_loops}")
    print(f"  Supervision steps: {config.n_sup_train}")
    print(f"  Model dim: {config.n_embd}")
    print(f"  Loss: 3^i weighting + improvement reward")
    print()

    model = GPT(config)
    model.init_weights()

    # Lower LR for deeper model
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.002)

    idx = torch.randint(0, 100, (4, 64))  # Larger batch
    targets = torch.randint(0, 100, (4, 64))

    print("Training for 150 iterations...")
    print()

    best_refinement = 0

    for i in range(150):
        lr_mult = min((i + 1) / 75, 1.0)  # 75-step warmup
        for param_group in optimizer.param_groups:
            param_group['lr'] = 0.002 * lr_mult

        optimizer.zero_grad()
        loss, aux = model(idx, targets)

        if torch.isnan(loss):
            print(f"âŒ NaN at iteration {i}")
            break

        loss.backward()

        grad_norm = sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None)**0.5
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        # Evaluate refinement every 15 steps
        if i % 15 == 0:
            model.eval()
            with torch.no_grad():
                x = model.transformer.wte(idx)
                x = torch.nn.functional.rms_norm(x, (x.size(-1),))
                y, z = x.clone(), torch.zeros_like(x)

                losses = []
                for sup_step in range(config.n_sup_train):
                    for block in model.transformer.h:
                        cos_sin = model.cos[:, :idx.size(1)], model.sin[:, :idx.size(1)]
                        y, z = block(x, y, z, cos_sin, kv_cache=None)

                    logits = model.lm_head(torch.nn.functional.rms_norm(y, (y.size(-1),)))
                    logits = 15 * torch.tanh(logits / 15)
                    step_loss = torch.nn.functional.cross_entropy(
                        logits.view(-1, logits.size(-1)),
                        targets.view(-1),
                        ignore_index=-1
                    )
                    losses.append(step_loss.item())

            model.train()

            refinement = (losses[0] - losses[-1]) / losses[0] * 100
            if refinement > best_refinement:
                best_refinement = refinement

            is_improving = all(losses[j] >= losses[j+1] - 0.001 for j in range(len(losses)-1))
            status = "âœ…" if is_improving else "âš ï¸"

            print(f"Iter {i:3d}: {status} refinement={refinement:+.2f}% | grad={grad_norm:.3f}")
            print(f"         losses=[{', '.join([f'{l:.4f}' for l in losses])}]")

    print()
    print("="*80)
    print("FINAL EVALUATION")
    print("="*80)
    print()

    # Final comprehensive evaluation
    model.eval()
    with torch.no_grad():
        x = model.transformer.wte(idx)
        x = torch.nn.functional.rms_norm(x, (x.size(-1),))
        y, z = x.clone(), torch.zeros_like(x)

        final_losses = []
        for sup_step in range(config.n_sup_train):
            for block in model.transformer.h:
                cos_sin = model.cos[:, :idx.size(1)], model.sin[:, :idx.size(1)]
                y, z = block(x, y, z, cos_sin, kv_cache=None)

            logits = model.lm_head(torch.nn.functional.rms_norm(y, (y.size(-1),)))
            logits = 15 * torch.tanh(logits / 15)
            step_loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1
            )
            final_losses.append(step_loss.item())

    print(f"Final supervision losses:")
    for i, loss in enumerate(final_losses):
        if i > 0:
            delta = (final_losses[i-1] - loss) / final_losses[i-1] * 100
            print(f"  Step {i}: {loss:.6f} ({delta:+.2f}% vs previous)")
        else:
            print(f"  Step {i}: {loss:.6f} (initial)")

    print()
    final_refinement = (final_losses[0] - final_losses[-1]) / final_losses[0] * 100
    print(f"Total refinement: {final_refinement:.2f}%")
    print(f"Best refinement during training: {best_refinement:.2f}%")
    print()

    is_monotonic = all(final_losses[j] >= final_losses[j+1] for j in range(len(final_losses)-1))

    if is_monotonic and final_refinement > 5:
        print("ğŸ‰ EXCELLENT! TRM is working perfectly!")
        print("   âœ… Monotonic refinement across all supervision steps")
        print(f"   âœ… Strong refinement ({final_refinement:.2f}%)")
        print("   âœ… Each loop improves the output")
    elif final_refinement > 5:
        print("âœ… SUCCESS! TRM is working well!")
        print(f"   âœ… Strong refinement ({final_refinement:.2f}%)")
        print("   âš ï¸  Near-monotonic (minor fluctuations acceptable)")
    else:
        print("âš ï¸  TRM showing some refinement")
        print(f"   âš ï¸  Moderate refinement ({final_refinement:.2f}%)")
        print("   ğŸ’¡ Try: deeper model, more training, or adjust hyperparameters")

    print()
    print("="*80)
    print("KEY TAKEAWAYS")
    print("="*80)
    print()
    print("âœ… TRM mechanisms implemented:")
    print("   â€¢ Progressive refinement with y/z states")
    print("   â€¢ Hierarchical loss weighting (3^i)")
    print("   â€¢ Improvement reward bonus")
    print("   â€¢ Gradient flow without detachment")
    print("   â€¢ Gradient clipping for stability")
    print()
    print("ğŸ“Š Performance:")
    print(f"   â€¢ {final_refinement:.1f}% improvement from first to last supervision step")
    print(f"   â€¢ Stable training (no NaN/explosion)")
    print(f"   â€¢ Each recursive pass refines the output")
    print()
    print("ğŸš€ Your TRM model is ready for full-scale training!")

if __name__ == "__main__":
    main()
